> 配合[从马克洛夫模型到DPPO](https://zhuanlan.zhihu.com/p/111869532)食用
#### 如何看待强化学习中的V值和Q值？
其实V和Q的意义是类似的，唯一的不同是V是对状态节点的估算，Q是对动作节点的估算。
估算什么：估算从该节点，一直到最终状态，能够获得的奖励的总和的平均值。
书面化一点：V值帮助智能体评估不同状态的优劣。& Q值帮助智能体在每个状态选择最优动作。

#### 蒙特卡洛（MC）和时序差分（TD）
MC：探索状态S执行所有动作的价值g【g是探索到最终状态得到的】，将g值平均便得到状态S的V值
MC的弊端：
1. MC算法相对动态规划，会有点不那么准。因为MC每一次的路径都是不一样的。 
2. 如果环境的状态空间非常大，或者最终状态只有非常小的概率达到。那么MC算法将会很难处理。
TD：探索状态S执行所有动作的价值g【g是探索N步得到的】，将g值平均便得到状态S的V值
**TD更厉害的是，在很多时候，我们并不需要一直到最后，我们可以先用后面的估算，然后调整当前状态。**

#### Qlearning 和 SARSA
我们希望通过TD直接预估Q值【而非从V值转为Q值】，因为我们最终还是希望智能体来选择动作。
SARSA的想法是：**用同一个策略下产生的动作A的Q值替代V(St+1)**。
Qlearning：**我们要用所有动作的Q值最大值来代替V(St+1)**。【状态S下有很多动作，我们要选取其中一个动作来代替V值】
整个思路： 
1. Qlearning和SARSA都是基于TD(0)的。不过在之前的介绍中，我们用TD(0)估算状态的V值。而Qlearning和SARSA估算的是动作的Q值。 
2. Qlearning和SARSA的核心原理，是用下一个状态St+1的V值，估算Q值。 
3. 既要估算Q值，又要估算V值会显得比较麻烦。所以我们用下一状态下的某一个动作的Q值，来代表St+1的V值。 
4. Qlearning和SARSA唯一的不同，就是用什么动作的Q值替代St+1的V值。 
   - 在相同策略下产生的动作At+1。这就是SARSA。 
   - 选择能够产生最大Q值的动作At+1。这就是Qlearning。

 #### DQN -> double DQN[two networks get the minimum]
 假设我们需要更新当前状态St下的某动作A的Q值：Q(S,A),我们可以这样做： 
 1. 执行A，往前一步，到达St+1; 
 2. 把St+1输入Q网络，计算St+1下所有动作的Q值； 
 3. 获得最大的Q值加上奖励R作为更新目标； 
 4. 计算损失 - Q(S,A)相当于有监督学习中的logits - maxQ(St+1) + R 相当于有监督学习中的lables - 用mse函数，得出两者的loss 
 5. 用loss更新Q网络。

**也就是，我们用Q网络估算出来的两个相邻状态的Q值，他们之间的距离，就是一个r的距离。**

#### 策略梯度(PG)
[PG](https://zhuanlan.zhihu.com/p/110881517)
PG的基本思想是通过神经网络magic(s) = a，PG用MC的G值来更新网络。也就是说，PG会让智能体一直走到最后。然后通过回溯计算G值。
这里的G就是对于状态S，选择了A的评分。也就是说， 
- 如果G值正数，那么表明选择A是正确的，我们希望神经网络输出A的概率增加。(鼓励) 
- 如果G是负数，那么证明这个选择不正确，我们希望神经网络输出A概率减少。(惩罚) 
- 而G值的大小，就相当于鼓励和惩罚的力度了。

#### ActorCritic
![DQN框架图](image.png)
+ 为什么我们不让critic网络用Q值作为更新值？ 避免正数陷阱--以免一个很好的动作没有被采样到
总结一下TD-error的知识[TD-error可以理解为G]：
1. 为了避免正数陷阱，我们希望Actor的更新权重有正有负。因此，我们把Q值减去他们的均值V。有：Q(s,a)-V(s)
2. 为了避免需要预估V值和Q值，我们希望把Q和V统一；由于Q(s,a) = gamma * V(s') + r - V(s)。所以我们得到TD-error公式： TD-error = gamma * V(s') + r - V(s)
3. TD-error就是Actor更新策略时候，带权重更新中的权重值；
4. 现在Critic不再需要预估Q，而是预估V。而根据马可洛夫链所学，我们知道TD-error就是Critic网络需要的loss，也就是说，Critic函数需要最小化TD-error。
**但要注意，我们需要先更新Critic，并计算出TD-error。再用TD-error更新Actor。**

#### PPO
概念说明：
+ 行为策略——不是当前策略，用于产出数据 - 目标策略——会更新的策略，是需要被优化的策略
  + 如果两个策略是同一个策略，那么我们称为On Policy，在线策略。
  + 如果不是同一个策略，那么Off Policy，离线策略。
+ 举例
  + 如果我们在智能体和环境进行互动时产生的数据打上一个标记。标记这是第几版本的策略产生的数据,例如 1， 2... 10。现在我们的智能体用的策略 10，需要更新到 11。如果算法只能用 10版本的产生的数据来更新，那么这个就是在线策略；如果算法允许用其他版本的数据来更新，那么就是离线策略。
+ 重要性采样[PPO](https://zhuanlan.zhihu.com/p/111049450)
  + 对于离线更新策略而言，因为行为策略与更新策略不同， 因此通常情况下，经验作用不大
  + PPO使用重要性采样【就是目标策略出现动作a的概率 除以 行为策略出现a的概率】，提升重要策略选中的概率
+ 总结 
  + 我们可以用AC来解决连续型控制问题。方法是输入mu和sigma，构造一个正态分布来表示策略； 
  + PPO延展了TD(0)，变成TD(N)的N步更新； 
  + AC是一个在线算法，但为了增加AC的效率，我们希望把它变成一个离线策略，这样就可以多次使用数据了。为了解决这个问题，PPO使用了重要性采样。

#### DDPG[解决DQN连续控制型问题而生]
什么叫deterministic: 其实DDPG也是解决连续控制型问题的的一个算法，不过和PPO不一样，PPO输出的是一个策略，也就是一个概率分布，而DDPG输出的直接是一个动作。
Critic
+ Critic网络的作用是预估Q，虽然它还叫Critic，但和AC中的Critic不一样，这里预估的是Q不是V；
+ 注意Critic的输入有两个：动作和状态，需要一起输入到Critic中；
+ Critic网络的loss其还是和AC一样，用的是TD-error。这里就不详细说明了，我详细大家学习了那么久，也知道为什么了。
Actor
+ 和AC不同，Actor输出的是一个动作；
+ Actor的功能是，输出一个动作A，这个动作A输入到Crititc后，能够获得最大的Q值。
+ 所以Actor的更新方式和AC不同，不是用带权重梯度更新，而是用梯度上升。

总结：
1. DDPG源于DQN，而不是源于AC。这一点要搞清楚。 
2. Actor用的是梯度上升，而不是带权重的梯度更新； 
3. 虽然Critic和AC一样，都是用td-error来更新；但AC的critic预估的是V，DDPG预估的是Q

#### TD3[DDPG优化版本]
TD3对DDPG的优化，主要包括三个部分： 
1. 用类似双Q网络的方式，解决了DDPG中Critic对高估动作Q值的问题； 
2. 延迟actor更新，让actor的训练更加稳定； 让critic更加确定之后，再让actor进行梯度上升，选择Q值最高的动作。
3. 在target_actor中加上噪音，增加算法稳定性。
   1. 在跑游戏的时候，我们同样加上了了noise。这个时候的noise是为了更充分地开发整个游戏空间。
   2. 计算target的时候，actor加上noise，是为了预估更准确，网络更有健壮性。
   3. 更新actor的时候，我们不需要加上noise，这里是希望actor能够寻着最大值。加上noise并没有任何意义。

#### A3C
在算法没有更大进步的时候，有人就想出，如果我又多个智能体和环境进行互动，那么每个智能体都能产出数据，这些数据就可以一起给模型进行学习了。
架构图分为两个主要部分：Global Network（全局网络）和 worker（工人）:其实Global network和 worker都是一模一样的AC结构网络。全局网络并不直接参加和环境的互动，工人与环境有直接的互动，并且把学习到的东西，汇报给全局网络。
注意：
+ 在A3C中，worker不仅要和环境互动，产生数据，而且要自己从这些数据里面学习到“心得”。这里的所谓新的，其实就是计算出来的梯度；需要强调的是，**worker向全局网络汇总的是梯度，而不是自己探索出来的数据。**
+ worker向全局网络汇总梯度之后，并应用在全局网络的参数后，全局网络会把当前学习到的最新版本的参数，直接给worker。

#### DPPO【核心就是多线程以及多线程之间的通信】
PPO解决了离线更新策略的问题，所以DPPO的工人只需要提供数据给全局网络，由全局网络从数据中直接学习。
[DPPO](https://zhuanlan.zhihu.com/p/111346592)